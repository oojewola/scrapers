
This scraper reads the already scraped profile links from the supplied csv file, opens each profile and scrapes relavant information
Info Scraped:
1). About
2). Experiences
3). Education
4). Location
5). Contact info

All Necessary imports

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from bs4 import BeautifulSoup as bs
import re as re
import time
import pandas as pd
from selenium.common.exceptions import NoSuchElementException
df = pd.read_csv('data/kelley_ds.csv')
def login(driver, email=None, password=None, timeout=10):
    driver.get("https://www.linkedin.com/login")
    WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.ID, "username"))).send_keys(email)
    WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.ID, "password"))).send_keys(password + Keys.RETURN)
    
    if driver.current_url == 'https://www.linkedin.com/checkpoint/lg/login-submit':
        remember_prompt = WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.ID, 'remember-me-prompt__form-primary')))
        remember_prompt.submit()

    WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.CLASS_NAME, "global-nav__primary-link")))
driver = webdriver.Chrome()
email = "sarathsurendran86@gmail.com"
password = "Linkedin 2792"
login(driver, email, password)
profiles = df['Link'].tolist()
for profile in profiles:
    driver.get(profile)
    time.sleep(3)
    try:
        driver.find_element_by_class_name('pv-skill-categories-section__additional-skills').click()
        time.sleep(3)
        driver.find_element_by_class_name('pv-skill-categories-section__additional-skills').click()
    except NoSuchElementException:
        pass
    time.sleep(3)
    soup = bs(driver.page_source, 'html.parser')
    skills = soup.find_all('span', class_='pv-skill-category-entity__name-text')
    skills = [skill.text for skill in skills]
    print(skills)
    # df.loc[df['Link'] == profile, 'Skills'] = ', '.join(skills)
    # df.to_csv('data/kelley_ds.csv', index=False)
    time.sleep(3)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[18], line 6
      4 time.sleep(3)
      5 try:
----> 6     driver.find_element_by_class_name('pv-skill-categories-section__additional-skills').click()
      7     time.sleep(3)
      8     driver.find_element_by_class_name('pv-skill-categories-section__additional-skills').click()

AttributeError: 'WebDriver' object has no attribute 'find_element_by_class_name'
about_elem=driver.find_element(By.ID,"about")
bs_obj=bs(about_elem.get_attribute('outerHTML'),"html.parser")
spans=bs_obj.find_all('span',class_='visually-hidden')

print(spans)
[]
top_panels = driver.find_elements(By.CLASS_NAME,"artdeco-card")
name = top_panels[0].find_elements(By.TAG_NAME,"h1")[0].text
location = top_panels[0].find_element(By.TAG_NAME,"span").text
links=df["Link"]
count=0
for link in links:
    try:
        person = Person(linkedin_url=link, driver=driver, scrape=True)
        count+=1
        if count%10==0:
            break
        print(person)
    except Exception as e:
        print("Error scraping profile:", e)
        continue
person = Person("https://www.linkedin.com/in/andre-iguodala-65b48ab5", driver = driver, scrape=True)
location = ""  # Default to an empty string if location is not found

try:
    # Attempt to find the element containing the location information
    # Adjust the selector based on the consistent pattern you observe on the page
    location_element = driver.find_element(By.XPATH, "//div[contains(@class, 'mt2')]/span[contains(@class, 't-black--light')]")
    
    # Only proceed to extract text if the element is found
    if location_element:
        location = location_element.text.strip()

except NoSuchElementException:
    # Handle the case where the element is not found
    print("Location element not found.")
def focus(driver):
    driver.execute_script('alert("Focus window")')
    driver.switch_to.alert.accept()
import os
current_profile=driver.current_url

url = os.path.join(current_profile, "details/experience/")
driver.get(url)
focus(driver)

#email,contact,location,about,experience,education,skills,accomplishments,interests,groups,websites,linkedin_url
from bs4 import BeautifulSoup
html_content=driver.page_source

soup = BeautifulSoup(html_content, "html.parser")

for div in soup.find_all('div', class_='pvs-entity__sub-components'):
    div.decompose()

experience_section = soup.find('section', {'class': 'artdeco-card'})

jobs = experience_section.find_all('li', class_='pvs-list__paged-list-item')

experiences = []

for job in jobs:


    # Find spans for title, company, etc., ignoring those within job details
    job_main_details = job.find_all('span', class_='visually-hidden', recursive=True)
    details_list = [detail.get_text(strip=True) for detail in job_main_details]

    # Basic job details
    title = details_list[0] if len(details_list) > 0 else 'No Title'
    company = details_list[1] if len(details_list) > 1 else 'No Company'
    time_period = details_list[2] if len(details_list) > 2 else 'No Time Period'
    location = details_list[3] if len(details_list) > 3 else 'No Location'

    experiences.append({
        'title': title,
        'company': company,
        'time_period': time_period,
        'location': location
    })
